{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting SWC files to arrays\n",
    "\n",
    "We can't feed the SWC files directly into our neural network. First, we must connect the nodes, then somehow load them into numpy.\n",
    "\n",
    "In the interest of time, I will use already-existing Vaa3D plugins to first render 3D TIFF images, which can then be loaded with numpy.\n",
    "\n",
    "First, I want a list of file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swc_io import get_fnames_and_abspath_from_dir, swc_to_TIFF\n",
    "\n",
    "MEAN_CENTERED_CUBES_DIR = \"../data/06_centered_cubes\"\n",
    "CUBES_DIRECTORY = 'cubes_directory.txt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#with open(CUBES_DIRECTORY, 'w+') as cubes_directory:\n",
    "#    for cube_swc_path in abs_paths:\n",
    "#      cubes_directory.write(\"%s\\n\" % cube_swc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "vaa3d_path = filedialog.askopenfilename(title='Select compiled Vaa3d binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threader import multithreading\n",
    "\n",
    "#fnames, abs_paths = get_fnames_and_abspath_from_dir(BRANCHES_DIR)\n",
    "fnames, abs_paths = get_fnames_and_abspath_from_dir(MEAN_CENTERED_CUBES_DIR)\n",
    "assert(len(abs_paths)>0)\n",
    "\n",
    "vaa3d_repeated = [vaa3d_path for _ in range(len(abs_paths))]\n",
    "\n",
    "multithreading(swc_to_TIFF, zip(fnames, abs_paths, vaa3d_repeated), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName='SWC2npy')\n",
    "\n",
    "def combine_abstract_text(text1, text2):\n",
    "    \"\"\"markovify works best when each corpus is a single huge string. theefore,\n",
    "    reduce by key here\"\"\"\n",
    "    print(text1[:20], text2[:20])\n",
    "    return text1+text2\n",
    "\n",
    "def text_to_model(tup):\n",
    "    '''given an abstract, train a markov model\n",
    "\n",
    "    the 1 will be used for weights, later'''\n",
    "    _, text = tup\n",
    "    try:\n",
    "        # retain_original set to False to save lots of RAM\n",
    "        text_model = markovify.Text(text, state_size=STATE_SIZE, \\\n",
    "                                    retain_original=False)\n",
    "\n",
    "        # class is not serializable, so extract json first\n",
    "        # this makes a Text type object, so we coerce to str\n",
    "        model_json = str(text_model.to_json())\n",
    "        # TODO: change key for category\n",
    "        return _, model_json\n",
    "    except:\n",
    "        # TODO FIXME: many articles being lost due to illegal characters. see issue tracker.\n",
    "        print(\"model skipped in text_to_model:\", text[:50])\n",
    "        pass\n",
    "\n",
    "\n",
    "def combine_models(model_1, model_2):\n",
    "    \"\"\"this should come in with no key\"\"\"\n",
    "    print(\"mod1\", model_1[:10])\n",
    "    print(\"mod2\", model_2[:10])\n",
    "    jsons = []\n",
    "    for tup in unzipped:\n",
    "        tup = tup[0]  # unnest 1 level\n",
    "        if tup is None:\n",
    "            continue  # FIXME I don't know how these Nonetypes keep sneaking in\n",
    "        try:\n",
    "            _name, json = tup\n",
    "            jsons.append(json)\n",
    "        except ValueError:\n",
    "            print(\"combining failed\", tup)\n",
    "\n",
    "    # reconstitute classes from json\n",
    "    reconstituted_models = [markovify.Text.from_json(json_i) for json_i in jsons]\n",
    "\n",
    "    # hella redundant but combine() method only smashes 2 models at a time\n",
    "    combined_model = reconstituted_models.pop()\n",
    "    #weights = [1., 1.]\n",
    "    for model in reconstituted_models:\n",
    "        combined_model = markovify.combine([model, combined_model]) #, weights)\n",
    "        #weights[-1] += 1\n",
    "    combined_json = str(combined_model.to_json())\n",
    "    # TODO: change key for category\n",
    "    return \"_\", combined_json\n",
    "\n",
    "\n",
    "def model_to_json(model):\n",
    "    try:\n",
    "        model_name, model_json = model\n",
    "    except TypeError:  # TODO FIXME somehow STILL Nonetypes leaking through\n",
    "        print(\"model was a Nonetype, not saving squat\")\n",
    "        return None\n",
    "    if SAVE_MODELS:\n",
    "        fname = open('./models/{}_model.json'.format(model_name), 'w')\n",
    "        fname.write(model_json)\n",
    "        fname.close\n",
    "        print(\"wrote json to disk for model {}\".format(model_name))\n",
    "\n",
    "\n",
    "abstracts = sc.textFile(CUBES_DIRECTORY)\n",
    "abstracts = abstracts.map(split_line)\n",
    "abstracts = abstracts.filter(lambda tup: tup[1] is not None)\n",
    "abstracts = abstracts.filter(lambda tup: len(tup[1]) >= 12)\n",
    "#print(abstracts.take(1))\n",
    "abstracts = abstracts.reduceByKey(lambda text1, text2: text1+text2)\n",
    "abstracts.persist()  # do not lose RDD after next line\n",
    "print(\"# of words in each key/corpus: \", abstracts.map(lambda tup: len(tup[1])).collect())\n",
    "models = abstracts.map(text_to_model)\n",
    "#print(models.take(1))\n",
    "#combined_models = models.reduceByKey(combine_models)\n",
    "#print(combined_models.take(1))\n",
    "# I like this function better, except is isnt' working anymore and I can't figure out why\n",
    "#models.map(model_to_json)  \n",
    "# rdd.saveAsTextFile saves as tuples, which sucks\n",
    "if SAVE_MODELS:\n",
    "    model_dir = \"models/\"\n",
    "    from shutil import rmtree\n",
    "    rmtree(model_dir)\n",
    "    models.saveAsTextFile(model_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
