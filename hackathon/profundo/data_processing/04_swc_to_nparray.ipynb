{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting SWC files to arrays\n",
    "\n",
    "We can't feed the SWC files directly into our neural network. First, we must connect the nodes, then somehow load them into numpy.\n",
    "\n",
    "In the interest of time, I will use already-existing Vaa3D plugins to first render 3D TIFF images, which can then be loaded with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import markovify\n",
    "\n",
    "sc = SparkContext(appName='SparkSpeechGenereator')\n",
    "\n",
    "STATE_SIZE = 3\n",
    "SAVE_MODELS = True\n",
    "ABSTRACTS_FILE = \"./results/Num_anal_abstracts.csv\"\n",
    "\n",
    "def clean_text_for_markovify(text):\n",
    "    '''\n",
    "    Markovify has a few symbols it hates. we'll filter them\n",
    "    https://github.com/jsvine/markovify/blob/master/markovify/text.py#L104\n",
    "    '''\n",
    "    text = text.replace(\"'\", \"\").replace('\"', \"\").replace(\"(\", \"\")\\\n",
    "        .replace(\")\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_line(line):\n",
    "    \"\"\"all_abracts.csv has file_name, abstract\n",
    "    let's just grab abstracts for now\"\"\"\n",
    "    try:\n",
    "        strings = line.split(',', 1)\n",
    "        # TODO: return category of the text\n",
    "        return \"_\", str(strings[1])\n",
    "    except:\n",
    "        print(\"line skipped in split_line\")\n",
    "        pass\n",
    "\n",
    "def combine_abstract_text(text1, text2):\n",
    "    \"\"\"markovify works best when each corpus is a single huge string. theefore,\n",
    "    reduce by key here\"\"\"\n",
    "    print(text1[:20], text2[:20])\n",
    "    return text1+text2\n",
    "\n",
    "def text_to_model(tup):\n",
    "    '''given an abstract, train a markov model\n",
    "\n",
    "    the 1 will be used for weights, later'''\n",
    "    _, text = tup\n",
    "    try:\n",
    "        # retain_original set to False to save lots of RAM\n",
    "        text_model = markovify.Text(text, state_size=STATE_SIZE, \\\n",
    "                                    retain_original=False)\n",
    "\n",
    "        # class is not serializable, so extract json first\n",
    "        # this makes a Text type object, so we coerce to str\n",
    "        model_json = str(text_model.to_json())\n",
    "        # TODO: change key for category\n",
    "        return _, model_json\n",
    "    except:\n",
    "        # TODO FIXME: many articles being lost due to illegal characters. see issue tracker.\n",
    "        print(\"model skipped in text_to_model:\", text[:50])\n",
    "        pass\n",
    "\n",
    "\n",
    "def combine_models(model_1, model_2):\n",
    "    \"\"\"this should come in with no key\"\"\"\n",
    "    print(\"mod1\", model_1[:10])\n",
    "    print(\"mod2\", model_2[:10])\n",
    "    jsons = []\n",
    "    for tup in unzipped:\n",
    "        tup = tup[0]  # unnest 1 level\n",
    "        if tup is None:\n",
    "            continue  # FIXME I don't know how these Nonetypes keep sneaking in\n",
    "        try:\n",
    "            _name, json = tup\n",
    "            jsons.append(json)\n",
    "        except ValueError:\n",
    "            print(\"combining failed\", tup)\n",
    "\n",
    "    # reconstitute classes from json\n",
    "    reconstituted_models = [markovify.Text.from_json(json_i) for json_i in jsons]\n",
    "\n",
    "    # hella redundant but combine() method only smashes 2 models at a time\n",
    "    combined_model = reconstituted_models.pop()\n",
    "    #weights = [1., 1.]\n",
    "    for model in reconstituted_models:\n",
    "        combined_model = markovify.combine([model, combined_model]) #, weights)\n",
    "        #weights[-1] += 1\n",
    "    combined_json = str(combined_model.to_json())\n",
    "    # TODO: change key for category\n",
    "    return \"_\", combined_json\n",
    "\n",
    "\n",
    "def model_to_json(model):\n",
    "    try:\n",
    "        model_name, model_json = model\n",
    "    except TypeError:  # TODO FIXME somehow STILL Nonetypes leaking through\n",
    "        print(\"model was a Nonetype, not saving squat\")\n",
    "        return None\n",
    "    if SAVE_MODELS:\n",
    "        fname = open('./models/{}_model.json'.format(model_name), 'w')\n",
    "        fname.write(model_json)\n",
    "        fname.close\n",
    "        print(\"wrote json to disk for model {}\".format(model_name))\n",
    "\n",
    "\n",
    "abstracts = sc.textFile(ABSTRACTS_FILE)\n",
    "abstracts = abstracts.map(clean_text_for_markovify)\n",
    "abstracts = abstracts.map(split_line)\n",
    "abstracts = abstracts.filter(lambda tup: tup[1] is not None)\n",
    "abstracts = abstracts.filter(lambda tup: len(tup[1]) >= 12)\n",
    "#print(abstracts.take(1))\n",
    "abstracts = abstracts.reduceByKey(lambda text1, text2: text1+text2)\n",
    "abstracts.persist()  # do not lose RDD after next line\n",
    "print(\"# of words in each key/corpus: \", abstracts.map(lambda tup: len(tup[1])).collect())\n",
    "models = abstracts.map(text_to_model)\n",
    "#print(models.take(1))\n",
    "#combined_models = models.reduceByKey(combine_models)\n",
    "#print(combined_models.take(1))\n",
    "# I like this function better, except is isnt' working anymore and I can't figure out why\n",
    "#models.map(model_to_json)  \n",
    "# rdd.saveAsTextFile saves as tuples, which sucks\n",
    "if SAVE_MODELS:\n",
    "    model_dir = \"models/\"\n",
    "    from shutil import rmtree\n",
    "    rmtree(model_dir)\n",
    "    models.saveAsTextFile(model_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
